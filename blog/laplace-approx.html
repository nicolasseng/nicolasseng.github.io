<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- MathJax Config -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async>
    </script>

    <!-- Optional: Syntax Highlighting für Code -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js">
    </script>
    <script>hljs.highlightAll();</script>
    <title>Laplace Approximation for Neural Networks - Nicolas Seng</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #1d1d1f;
            background: #fbfbfd;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        nav {
            position: sticky;
            top: 0;
            background: rgba(251, 251, 253, 0.8);
            backdrop-filter: saturate(180%) blur(20px);
            border-bottom: 1px solid rgba(0, 0, 0, 0.1);
            z-index: 1000;
            padding: 0 22px;
        }

        .nav-container {
            max-width: 980px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            height: 48px;
        }

        .nav-name {
            font-size: 21px;
            font-weight: 600;
            color: #1d1d1f;
            letter-spacing: -0.02em;
        }

        .nav-links {
            display: flex;
            gap: 32px;
        }

        .nav-links a {
            color: #1d1d1f;
            text-decoration: none;
            font-size: 14px;
            font-weight: 400;
            opacity: 0.8;
            transition: opacity 0.3s ease;
        }

        .nav-links a:hover {
            opacity: 1;
        }

        .hero {
            max-width: 730px;
            margin: 80px auto 40px;
            padding: 0 22px;
        }

        .hero h1 {
            font-size: 48px;
            font-weight: 600;
            letter-spacing: -0.015em;
            line-height: 1.08;
            margin-bottom: 16px;
            color: #1d1d1f;
        }

        .meta {
            font-size: 17px;
            color: #6e6e73;
            margin-bottom: 40px;
        }

        .content {
            max-width: 730px;
            margin: 0 auto;
            padding: 0 22px 100px;
        }

        .content p {
            font-size: 19px;
            line-height: 1.58;
            font-weight: 400;
            color: #1d1d1f;
            margin-bottom: 28px;
        }

        .content h2 {
            font-size: 32px;
            font-weight: 600;
            letter-spacing: -0.01em;
            line-height: 1.125;
            margin-top: 56px;
            margin-bottom: 20px;
            color: #1d1d1f;
        }

        .content h3 {
            font-size: 24px;
            font-weight: 600;
            letter-spacing: -0.009em;
            margin-top: 40px;
            margin-bottom: 16px;
            color: #1d1d1f;
        }

        .content ul,
        .content ol {
            margin-left: 28px;
            margin-bottom: 28px;
        }

        .content li {
            font-size: 19px;
            line-height: 1.58;
            margin-bottom: 12px;
            color: #1d1d1f;
        }

        .content code {
            background: #f5f5f7;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 17px;
            color: #1d1d1f;
        }

        .content pre {
            background: #f5f5f7;
            padding: 20px;
            border-radius: 12px;
            overflow-x: auto;
            margin-bottom: 28px;
        }

        .content pre code {
            background: none;
            padding: 0;
            font-size: 15px;
        }

        .content blockquote {
            border-left: 4px solid #0066cc;
            padding-left: 24px;
            margin: 28px 0;
            color: #6e6e73;
            font-style: italic;
        }

        .content a {
            color: #0066cc;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .content a:hover {
            color: #0077ed;
        }

        .content img {
            max-width: 100%;
            height: auto;
            border-radius: 12px;
            margin: 28px 0;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 40px;
            color: #0066cc;
            text-decoration: none;
            font-size: 17px;
        }

        .back-link:hover {
            color: #0077ed;
        }

        footer {
            max-width: 980px;
            margin: 0 auto;
            padding: 40px 22px;
            border-top: 1px solid rgba(0, 0, 0, 0.1);
            font-size: 14px;
            color: #6e6e73;
            text-align: center;
        }

        footer a {
            color: #0066cc;
            text-decoration: none;
        }

        footer a:hover {
            color: #0077ed;
        }

        @media (max-width: 768px) {
            .hero h1 {
                font-size: 36px;
            }

            .content h2 {
                font-size: 28px;
            }

            .content h3 {
                font-size: 21px;
            }

            .nav-links {
                gap: 20px;
            }

            .nav-links a {
                font-size: 12px;
            }
        }
    </style>
</head>

<body>
    <nav>
        <div class="nav-container">
            <div class="nav-name">Nicolas Seng</div>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="../index.html#blog">Blog</a>
                <a href="../index.html#cv">CV</a>
                <a href="https://github.com/nicolasseng" target="_blank">GitHub</a>
            </div>
        </div>
    </nav>

    <div class="hero">
        <h1>Laplace Approximation for Neural Networks · Theory</h1>
        <div class="meta">Soon</div>
    </div>

    <div class="content">
        <a href="../index.html#blog" class="back-link">← Back to Blog</a>

        <!-- DEIN CONTENT STARTET HIER -->

        <p>
            This is the first part of a step-by-step guide on how to train a neural network on a standard regression
            dataset
            and
            then try to approximate the posterior distribution by fitting a Gaussian centered at the
            maximum-a-posteriori estimate. In this part, we focus on the theory.
        </p>

        <h2>Why should we care?</h2>

        <p>
            Training a neural network on a dataset yields a point estimate for the weights $\theta$ given some data
            $D:=\{(x_i \in \mathbb{R}^{d_x}, y_i \in \mathbb{R}^{d_y})\}_{i=1}^{N}$.
            This is equivalent to statistical risk minimization:
        </p>
        $$
        \theta_{\text{MAP}}=\arg \underset{\theta}{\min} \, \mathcal{L}(D;\theta)
        $$
        <p>
            The result is a set of weights that minimize the loss function w.r.t your data. For new unseen data
            points, your model may carry a lot of uncertainty in the prediction.
        </p>
        <p>
            Suppose you train a neural network to predict house prices and your trained model predicts a price of
            200.000€ for a given house. How certain is the model about this prediction? Is it within a range of
            10.000€? 100.000€?
        </p>

        <p>
            This is particulary interesting for safety-critical applications, such as autonomous driving or medical
            diagnosis. Uncertainty quantification tries to solve this problem.
        </p>

        <h2>Bayes' Theorem</h2>
        <p>
            The posterior distribution can be calculated according to Bayes' theorem:
        </p>
        $$
        \underbrace{p(\theta | \mathcal{D})}_{\text{Posterior}} =
        \frac{\overbrace{p(\mathcal{D} | \theta)}^{\text{Likelihood}} \cdot
        \overbrace{p(\theta)}^{\text{Prior}}}
        {\underbrace{p(\mathcal{D})}_{\text{Evidence}}} \propto p(\mathcal{D}|\theta) \cdot p(\theta)
        $$
        <p>
            stating how prior knowledge and observed data result in the posterior distribution.
        </p>
        <p>
            In order to find the posterior distribution of our weights, we need to maximize $p(\theta|\mathcal{D})$
            w.r.t the weights. Since this is proportional to $p(D|\theta) \cdot p(\theta)$ and maximizing a product is
            challenging, we rather work in log-space and maximize the sum:
        </p>
        $$
        \theta_{\text{MAP}}=\arg \underset{\theta}{\max} \, \log(p(\mathcal{D}|\theta))+\log(p(\theta))
        $$

        <h2>The Laplace Approximation</h2>

        <p>
            The Laplace approximation tries to approximate the posterior distribution by fitting a Gaussian centered at
            the point-estimate. This Gaussian is characterized by mean $\theta_\text{MAP}$ and $\Sigma:=(\nabla^2_\theta
            \mathcal{L}(\mathcal{D},\theta)|_{\theta_{\text{MAP}}})^{-1}$ as the covariance matrix.
        </p>
        <p>
            This can be obtained by using the second-order Taylor expansion of the posterior at $\theta_\text{MAP}$:
        </p>
        $$
        p(\theta|D)=\frac{1}{Z}p(\mathcal{D}|\theta) \cdot p(\theta) \approx
        \frac{\det(\Lambda)^{\frac{1}{2}}}{(2\pi)^{\frac{d}{2}}}
        \text{exp}\left(-\frac{1}{2}(\theta-\theta_{\text{MAP}})^T\Lambda(\theta-\theta_{\text{MAP}})\right),
        $$
        <p>
            with $\Lambda:=-\nabla^2 p(\mathcal{D}|\theta) \cdot p(\theta)|_{\theta_{\text{MAP}}}$ being the Hessian
            matrix of
            the
            unnormalized posterior evaluated at $\theta_{\text{MAP}}$. A more detailed proof can be found <a
                href="https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html"> here</a> and
            here: <a href="https://arxiv.org/pdf/2106.14806"> Daxberger et al.</a>
        </p>

        <!-- DEIN CONTENT ENDET HIER -->

        <a href="../index.html#blog" class="back-link">← Back to Blog</a>
    </div>

    <footer>
        <p>© 2025 Nicolas Seng · <a href="https://github.com/nicolasseng">GitHub</a></p>
    </footer>
</body>

</html>